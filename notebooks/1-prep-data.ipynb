{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "590aeabc-8f00-4170-834c-9fbd863eed84",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Preparation for Language Model Training\n",
    "\n",
    "This notebook demonstrates the process of preparing and formatting training data for a language model using the Databricks Dolly dataset and custom QA pairs.\n",
    "\n",
    "## 1. Import Required Libraries\n",
    "\n",
    "- Imports necessary Python libraries for data processing and model handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf29a460-8a21-49ee-9656-e99132bf2c29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from typing import Dict, List, Any\n",
    "from dotenv import load_dotenv\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d2b50e-c136-4668-8779-f8bcec0aadfa",
   "metadata": {},
   "source": [
    "## 2. HuggingFace Authentication Setup\n",
    "\n",
    "- Sets up authentication with HuggingFace using environment variables and API token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08dfc34-05e2-4d4a-9620-0da7ad6fda30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_dotenv('../end.local')\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "if not HF_TOKEN:\n",
    "    raise ValueError(\"HF_TOKEN not found in environment variables\")\n",
    "\n",
    "!huggingface-cli login --token {HF_TOKEN}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7390f95-9048-4917-98ef-51b06ba1129c",
   "metadata": {},
   "source": [
    "## 3. Load and Combine Datasets\n",
    "- Loads the Dolly dataset and custom QA pairs, then combines them into a single dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8978c84d-24b1-4e54-83b8-e2c712db3f21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\")\n",
    "qa_dataset = load_dataset('json', data_files='../data/qa_pairs.jsonl', split='train')\n",
    "\n",
    "def convert_qa_to_dolly_format(example):\n",
    "    return {\n",
    "        \"instruction\": example[\"question\"],\n",
    "        \"context\": \"\",  \n",
    "        \"response\": example[\"answer\"],\n",
    "        \"category\": \"closed_qa\"  \n",
    "    }\n",
    "\n",
    "qa_dataset_converted = qa_dataset.map(convert_qa_to_dolly_format)\n",
    "qa_dataset_converted = qa_dataset_converted.remove_columns(['question', 'answer', 'chunk_index'])\n",
    "\n",
    "combined_dataset = concatenate_datasets([dataset['train'], qa_dataset_converted])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f19caf-5fad-4ac1-bd4e-7b1d6d189f74",
   "metadata": {},
   "source": [
    "## 4. Dataset Splitting\n",
    "- Splits the combined dataset into train (80%), validation (10%), and test (10%) sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f78b1c-9962-43c4-b603-f8c99a5f6a95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "TEST_SIZE = 0.2\n",
    "VALIDATION_SIZE = 0.5\n",
    "\n",
    "initial_split = combined_dataset.train_test_split(\n",
    "    test_size=TEST_SIZE,\n",
    "    seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "validation_test_split = initial_split['test'].train_test_split(\n",
    "    test_size=VALIDATION_SIZE,  \n",
    "    seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "final_dataset = {\n",
    "    'train': initial_split['train'],\n",
    "    'validation': validation_test_split['train'],\n",
    "    'test': validation_test_split['test']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6f6377-f092-4014-81a8-39634c85da19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(next(iter(example for example in final_dataset['train'] if example['context'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3233e23b-71ef-4b74-8677-a7c7c47fd5cf",
   "metadata": {},
   "source": [
    "## 5. Message formatting\n",
    "- Formats the data into a structured conversation format with user instructions and assistant responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5757eb0b-aef3-4220-bfe1-9e6087d4204d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_instruction(instruction: str, context: str, response: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Format instruction, context and response into a structured conversation format.\n",
    "    \"\"\"\n",
    "    user_content = (\n",
    "        f\"Instruction: {instruction}\\n\"\n",
    "        f\"Context: {context.strip() if context and context.strip() else 'N/A'}\"\n",
    "    )\n",
    "    \n",
    "    response_data = {\n",
    "        \"response\": response,\n",
    "        \"metadata\": {\n",
    "            \"has_context\": bool(context and context.strip()),\n",
    "            \"input_type\": \"text\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    assistant_content = json.dumps(\n",
    "        response_data, \n",
    "        ensure_ascii=False, \n",
    "        indent=2\n",
    "    )\n",
    "\n",
    "    return [\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "    ]\n",
    "\n",
    "\n",
    "def process_datasets(dataset: Dict[str, Dataset], random_seed: int = 42, verbose: bool = False) -> Dict[str, Dataset]:\n",
    "    \"\"\"\n",
    "    Process all datasets (train, validation, test) at once.\n",
    "    \"\"\"\n",
    "    COLUMNS_TO_REMOVE = ['instruction', 'context', 'response', 'category']\n",
    "    \n",
    "    processed_datasets = {}\n",
    "    \n",
    "    for split_name, split_data in dataset.items():\n",
    "        if verbose:\n",
    "            print(f\"Processing {split_name} dataset...\")\n",
    "            \n",
    "        processed_datasets[split_name] = (\n",
    "            split_data.shuffle(seed=random_seed)\n",
    "            .map(\n",
    "                lambda x: {\n",
    "                    \"messages\": format_instruction(\n",
    "                        x[\"instruction\"],\n",
    "                        x[\"context\"],\n",
    "                        x[\"response\"]\n",
    "                    )\n",
    "                }\n",
    "            )\n",
    "            .remove_columns(COLUMNS_TO_REMOVE)\n",
    "        )\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"{split_name} dataset processed. Size: {len(processed_datasets[split_name])}\")\n",
    "    \n",
    "    return processed_datasets\n",
    "\n",
    "# Process datasets\n",
    "train_dataset = process_dataset(final_dataset['train'])\n",
    "validation_dataset = process_dataset(final_dataset['validation'])\n",
    "test_dataset = process_dataset(final_dataset['test'])\n",
    "\n",
    "# Display sample output\n",
    "print(\"\\nSample of transformed data:\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df725d49-b6cd-45b7-b9e9-052c05f8086c",
   "metadata": {},
   "source": [
    "## 6. Save Datasets\n",
    "- Processes the formatted data and saves it into JSON files for each split (train/validation/test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e265345-7dba-4224-a412-46983ddc2931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_datasets_to_json(dataset_name, datasets, base_path=\"../data\"):\n",
    "    \"\"\"\n",
    "    Save datasets to JSON files in organized directory structure.\n",
    "    \"\"\"\n",
    "    \n",
    "    data_folder = os.path.join(base_path, dataset_name)\n",
    "    os.makedirs(data_folder, exist_ok=True)\n",
    "    \n",
    "    json_paths = {}\n",
    "\n",
    "    for split_name, dataset in datasets.items():\n",
    "        split_dir = os.path.join(data_folder, split_name)\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "        \n",
    "        json_path = os.path.join(split_dir, f\"{split_name}_dataset.json\")\n",
    "        dataset.to_json(json_path)\n",
    "        \n",
    "        json_paths[split_name] = json_path\n",
    "        print(f\"{split_name.capitalize()} data saved to: {json_path}\")\n",
    "    \n",
    "    print(f\"\\nAll datasets saved to: {data_folder}\")\n",
    "    return data_folder, json_paths\n",
    "\n",
    "# Store dataset\n",
    "DATASET_NAME = \"dolly_dataset\"\n",
    "\n",
    "data_folder, json_paths = save_datasets_to_json(\n",
    "    dataset_name=DATASET_NAME,\n",
    "    datasets={\n",
    "        'train': train_dataset,\n",
    "        'validation': validation_dataset,\n",
    "        'test': test_dataset\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbf564a-0437-4355-8f11-b66850d9dca9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store data_folder\n",
    "%store json_paths"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3-env",
   "language": "python",
   "name": "llama3-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
